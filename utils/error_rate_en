#!/usr/bin/env python3
# coding=utf8
# Copyright  2022  Zhenxiang MA, Jiayu DU (SpeechColab)

import sys, os
import argparse
from typing import Iterable
import json, csv
import logging
logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='[%(levelname)s] %(message)s')

import pynini
from pynini.lib import pynutil

# reference: https://github.com/kylebgorman/pynini/blob/master/pynini/lib/edit_transducer.py
# to import original lib:
#     from pynini.lib.edit_transducer import EditTransducer
class EditTransducer:
    DELETE = "<delete>"
    INSERT = "<insert>"
    SUBSTITUTE = "<substitute>"

    def __init__(self,
        symbol_table,
        vocab: Iterable[str],
        edit_ext: pynini.FstLike,
        insert_cost: float = 1.0,
        delete_cost: float = 1.0,
        substitute_cost: float = 1.0,
        bound: int = 0,
    ) :
        # Left factor; note that we divide the edit costs by two because they also
        # will be incurred when traversing the right factor.
        sigma = pynini.union(
            *[ pynini.accep(token, token_type = symbol_table) for token in vocab ], 
        ).optimize()

        insert = pynutil.insert(f"[{self.INSERT}]", weight=insert_cost / 2)
        delete = pynini.cross(
            sigma,
            pynini.accep(f"[{self.DELETE}]", weight=delete_cost / 2)
        )
        substitute = pynini.cross(
            sigma, pynini.accep(f"[{self.SUBSTITUTE}]", weight=substitute_cost / 2)
        )

        edit = pynini.union(insert, delete, substitute).optimize()

        if bound:
            sigma_star = pynini.closure(sigma)
            self._e_i = sigma_star.copy()
            for _ in range(bound):
                self._e_i.concat(edit.ques).concat(sigma_star)
        else:
            self._e_i = edit.union(sigma).closure()

        self._e_i.optimize()
        self._e_o = EditTransducer._right_factor(self._e_i, edit_ext)

    @staticmethod
    def _right_factor(ifst: pynini.Fst, edit_ext: pynini.FstLike) -> pynini.Fst:
        ofst = pynini.invert(ifst)
        syms = pynini.generated_symbols()
        insert_label = syms.find(EditTransducer.INSERT)
        delete_label = syms.find(EditTransducer.DELETE)
        pairs = [(insert_label, delete_label), (delete_label, insert_label)]
        right_factor = ofst.relabel_pairs(ipairs=pairs)

        right_factor_aux = pynini.union(right_factor, edit_ext).closure().optimize()
        #print('R_start:', right_factor_aux.start())
        #print('R:', right_factor_aux)
        return right_factor_aux

    def create_lattice(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> pynini.Fst:
        lattice = (iexpr @ self._e_i) @ (self._e_o @ oexpr)
        EditTransducer.check_wellformed_lattice(lattice)
        return lattice

    @staticmethod
    def check_wellformed_lattice(lattice: pynini.Fst) -> None:
        if lattice.start() == pynini.NO_STATE_ID:
            raise RuntimeError("Edit distance composition lattice is empty.")

    def compute_distance(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> float:
        lattice = self.create_lattice(iexpr, oexpr)
        # The shortest cost from all final states to the start state is
        # equivalent to the cost of the shortest path.
        start = lattice.start()
        return float(pynini.shortestdistance(lattice, reverse=True)[start])
  
    def compute_alignment(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> pynini.FstLike:
        lattice = self.create_lattice(iexpr, oexpr)
        alignment = pynini.shortestpath(lattice, nshortest=1, unique=True)
        return alignment.optimize()


class ErrorStats:
    def __init__(self):
        self.num_ref_utts = 0
        self.num_hyp_utts = 0
        self.num_eval_utts = 0 # in both ref & hyp
        self.num_hyp_without_ref = 0

        self.C = 0
        self.S = 0
        self.I = 0
        self.D = 0
        self.token_error_rate = 0.0

        self.num_utts_with_error = 0
        self.sentence_error_rate = 0.0
    
    def to_json(self):
        #return json.dumps(self.__dict__, indent=4)
        return json.dumps(self.__dict__)
    
    def to_kaldi(self):
        info = (
            F'%WER {self.token_error_rate:.2f} [ {self.S + self.D + self.I} / {self.C + self.S + self.D}, {self.I} ins, {self.D} del, {self.S} sub ]\n'
            F'%SER {self.sentence_error_rate:.2f} [ {self.num_utts_with_error} / {self.num_eval_utts} ]\n'
        )
        return info
    
    def to_summary(self):
        summary = (
            '==================== Overall Statistics ====================\n'
            F'num_ref_utts: {self.num_ref_utts}\n'
            F'num_hyp_utts: {self.num_hyp_utts}\n'
            F'num_hyp_without_ref: {self.num_hyp_without_ref}\n'
            F'num_eval_utts: {self.num_eval_utts}\n'
            F'sentence_error_rate: {self.sentence_error_rate:.2f}%\n'
            F'token_error_rate: {self.token_error_rate:.2f}%\n'
            F'token_stats:\n'
            F'  - tokens:{self.C + self.S + self.D:>7}\n'
            F'  - edits: {self.S + self.I + self.D:>7}\n'
            F'  - cor:   {self.C:>7}\n'
            F'  - sub:   {self.S:>7}\n'
            F'  - ins:   {self.I:>7}\n'
            F'  - del:   {self.D:>7}\n'
            '============================================================\n'
        )
        return summary


class Utterance:
    def __init__(self, uid, text):
        self.uid = uid
        self.text = text


def LoadKaldiArc(filepath):
    utts = {}
    with open(filepath, 'r', encoding='utf8') as f:
        for line in f:
            line = line.strip()
            if line:
                cols = line.split(maxsplit=1)
                assert(len(cols) == 2 or len(cols) == 1)
                uid = cols[0]
                text = cols[1] if len(cols) == 2 else ''
                if utts.get(uid) != None:
                    raise RuntimeError(F'Found duplicated utterence id {uid}')
                utts[uid] = Utterance(uid, text)
    return utts


def Tokenize(text, tokenizer = 'whitespace'):
    if tokenizer == 'whitespace':
        return text.strip().split()
    elif tokenizer == 'char':
        return [ c for c in text.strip().replace(' ', '') ]
    else:
        raise RuntimeError(F'ERROR: Unsupported tokenizer {tokenizer}')


def tokens_without_hyphen(token : str):
    # 'T-SHIRT' should also introduce new words into vocabulary, e.g.:
    #   1. 'T' & 'SHIRT'
    #   2. 'TSHIRT'
    assert('-' in token)
    v = token.split('-')
    v.append(token.replace('-', ''))
    return v


def LoadGLM(rel_path):
    '''
    glm.csv:
        I'VE,I HAVE
        GOING TO,GONNA
        ...
        T-SHIRT,T SHIRT,TSHIRT

    glm:
        {
            '<RULE_00000>': ["I'VE", 'I HAVE'],
            '<RULE_00001>': ['GOING TO', 'GONNA'],
            ...
            '<RULE_99999>': ['T-SHIRT', 'T SHIRT', 'TSHIRT'],
        }
    '''
    logging.info(f'Loading GLM from {rel_path} ...')

    abs_path = os.path.dirname(os.path.abspath(__file__)) + '/' + rel_path
    reader = list(
        csv.reader(open(abs_path, encoding="utf-8"), delimiter=',')
    )

    glm = {}
    for k, rule in enumerate(reader):
        rule_name = f'<RULE_{k:06d}>'
        glm[rule_name] = [ phrase.strip() for phrase in rule ]
    logging.info(f'  #rule: {len(glm)}')

    return glm


def SymbolEQ(symtab, i1, i2):
    return symtab.find(i1).strip('#') == symtab.find(i2).strip('#')


def PrintSymbolTable(symbol_table: pynini.SymbolTable):
    print('SYMBOL_TABLE:')
    for k in range(symbol_table.num_symbols()):
        sym = symbol_table.find(k)
        assert(symbol_table.find(sym) == k) # symbol table's find can be used for bi-directional lookup (id <-> sym)
        print(k, sym)
    print()


def BuildSymbolTable(*vocabs) -> pynini.SymbolTable :
    logging.info('Building symbol table ...')
    symbol_table = pynini.SymbolTable()
    symbol_table .add_symbol('<epsilon>')

    for vocab in vocabs:
        for token in vocab:
            symbol_table.add_symbol(token)
    logging.info(f'  #symbols: {symbol_table.num_symbols()}')

    return symbol_table


def BuildGLMTagger(glm, symbol_table):
    logging.info('Building GLM tagger ...')
    rule_taggers = []
    for rule_tag, rule in glm.items():
        for phrase in rule:
            rule_taggers.append(
                (
                    pynutil.insert(pynini.accep(rule_tag, token_type = symbol_table)) +
                    pynini.accep(phrase, token_type = symbol_table) +
                    pynutil.insert(pynini.accep(rule_tag, token_type = symbol_table))
                )
            )

    alphabet = pynini.union(
        *[ pynini.accep(sym, token_type = symbol_table) for k, sym in symbol_table if k != 0 ] # non-epsilon
    ).optimize()

    tagger = pynini.cdrewrite(pynini.union(*rule_taggers), '', '', alphabet.closure()).optimize() # could be slow with large vocabulary
    return tagger


def PrintPrettyAlignment(raw, hyp, ref, edits, stream = sys.stderr):
    def fullwidth_char(c):
        return True if (c >= '\u4e00') and (c <= '\u9fa5') else False
    
    def token_width(token: str):
        n = 0
        for c in token:
            n += (2 if fullwidth_char(c) else 1)
        return n

    H = '  HYP# : '
    R = '  REF  : '
    E = '  EDIT : '

    ref = Tokenize(ref)
    hyp = Tokenize(hyp)
    rpos, hpos = 0, 0
    for e in edits:
        if e == 'C' or e == 'S':
            r = ref[rpos]
            rpos += 1

            h = hyp[hpos]
            hpos += 1
        elif e == 'I':
            r = '*'

            h = hyp[hpos]
            hpos += 1
        elif e == 'D':
            r = ref[rpos]
            rpos += 1

            h = '*'
        else:
            raise RuntimeError(f'Incorrect edit type: {e}')

        if e == 'C':
            e = '' # don't bother printing 'CORRECT' edit

        nr, nh, ne = token_width(r), token_width(h), token_width(e)
        n = max(nr, nh, ne) + 1

        H += h + ' ' * (n-nh)
        R += r + ' ' * (n-nr)
        E += e + ' ' * (n-ne)
    assert(rpos == len(ref))
    assert(hpos == len(hyp))

    print('  HYP  : ' + raw, file = stream)
    print(H, file = stream)
    print(R, file = stream)
    print(E, file = stream)


def ComputeTokenErrorRate(c, s, i, d):
    assert((s + d + c) != 0)
    return 100.0 * (s + d + i) / (s + d + c)


def ComputeSentenceErrorRate(num_err_utts, num_utts):
    assert(num_utts != 0)
    return 100.0 * num_err_utts / num_utts


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--logk', type = int, default = 500, help = 'logging interval')
    parser.add_argument('--tokenizer', choices = ['whitespace', 'char'], default = 'whitespace', help = 'whitespace for WER, char for CER')
    parser.add_argument('--glm', type = str, default = 'glm_en.csv', help = 'glm')
    parser.add_argument('--ref', type = str, required = True, help = 'reference kaldi arc file')
    parser.add_argument('--hyp', type = str, required = True, help = 'hypothesis kaldi arc file')
    parser.add_argument('result_file', type = str)
    args = parser.parse_args()
    logging.info(args)

    stats = ErrorStats()

    logging.info('Loading REF & HYP ...')
    ref_utts = LoadKaldiArc(args.ref)
    hyp_utts = LoadKaldiArc(args.hyp)

    # check valid utterances in hyp that have matched non-empty reference
    uids = []
    for uid in sorted(hyp_utts.keys()):
        if uid in ref_utts.keys():
            if ref_utts[uid].text.strip(): # non-empty reference
                uids.append(uid)
            else:
                logging.warning(F'Found {uid} with empty reference, skipping...')
        else:
            logging.warning(F'Found {uid} without reference, skipping...')
            stats.num_hyp_without_ref += 1

    stats.num_hyp_utts  = len(hyp_utts)
    stats.num_ref_utts  = len(ref_utts)
    stats.num_eval_utts = len(uids)
    logging.info(f'  #hyp:{stats.num_hyp_utts}, #ref:{stats.num_ref_utts}, #utts_to_evaluate:{stats.num_eval_utts}')

    tokens = []
    for uid in uids:
        ref_tokens = Tokenize(ref_utts[uid].text)
        hyp_tokens = Tokenize(hyp_utts[uid].text)
        for t in ref_tokens + hyp_tokens:
            tokens.append(t)
            if '-' in t:
                tokens.extend(tokens_without_hyphen(t))
    vocab_from_utts = list(set(tokens))
    logging.info(f'  HYP&REF vocab size: {len(vocab_from_utts)}')


    assert(args.glm)
    glm = LoadGLM(args.glm)

    tokens = []
    for rule in glm.values():
        for phrase in rule:
            for t in Tokenize(phrase):
                tokens.append(t)
                if '-' in t:
                    tokens.extend(tokens_without_hyphen(t))
    vocab_from_glm = list(set(tokens))
    logging.info(f'  GLM vocab size: {len(vocab_from_glm)}')


    vocab = list(set(vocab_from_utts + vocab_from_glm))
    logging.info(f'Global vocab size: {len(vocab)}')


    symtab = BuildSymbolTable(
        vocab,
        [ x+'#' for x in vocab ], # auxiliary markers for alternative paths
        [ x for x in glm.keys() ], # GLM rule tags
    )
    #PrintSymbolTable(symtab)
    #symtab.write_text('symbol_table.txt')

    glm_tagger = BuildGLMTagger(glm, symtab)

    # auxiliary edit transducer allows 0-cost matches between token and its auxiliary form
    # e.g.: 'I' -> 'I#', 'AM' -> 'AM#'
    edit_transducer_aux = pynini.union(*[
        pynini.cross(
            pynini.accep(token,       token_type = symtab),
            pynini.accep(token + '#', token_type = symtab),
        ) for token in vocab
    ]).optimize().closure()

    edit_transducer = EditTransducer(
        symbol_table = symtab,            
        vocab = vocab,
        edit_ext = edit_transducer_aux,
    )


    logging.info('Evaluating error rate ...')
    fo = open(args.result_file, 'w+', encoding='utf8')
    ndone = 0
    for uid in uids:
        ref = ref_utts[uid].text
        hyp = hyp_utts[uid].text

        ref_fst = pynini.accep(' '.join(Tokenize(ref)), token_type = symtab)
        #print(ref_fst.string(token_type = symtab))

        raw_hyp_fst = pynini.accep(' '.join(Tokenize(hyp)), token_type = symtab)
        #print(raw_hyp_fst.string(token_type = symtab))

        # Say, we have
        #   GLM contains a rule: 
        #     RULE_001: "I'M" <-> "I AM"
        #   REF: HEY I AM HERE
        #   HYP: HEY I'M HERE
        #
        # We want to expand HYP with GLM rules(marked with auxiliary #)
        #   HYP#: HEY {I'M | I# AM#} HERE
        # REF is honored to keep its original form.
        #
        # This could be considered as a flexible on-the-fly TN towards HYP.


        # 1. GLM rule tagging:
        #   HEY I'M HERE
        # ->
        #   HEY <RULE_001> I'M <RULE_001> HERE
        lattice = (raw_hyp_fst @ glm_tagger).optimize()
        tagged_ir = pynini.shortestpath(lattice, nshortest=1, unique=True).string(token_type = symtab)
        #print(hyp_tagged)

        # 2. GLM rule expansion: 
        #   HEY <RULE_001> I'M <RULE_001> HERE
        # ->
        #   sausage-like fst: HEY {I'M | I# AM#} HERE
        tokens = tagged_ir.split()
        sausage = pynini.accep('', token_type = symtab)
        i = 0
        while i < len(tokens): # invariant: tokens[0, i) has been built into fst
            forms = []
            if '<RULE_' in tokens[i]:  # rule segment
                rule_name = tokens[i]
                rule = glm[rule_name]

                # pre-condition: i -> ltag
                for j in range(i+1, len(tokens)):
                    if '<RULE_' in tokens[j]:
                        break
                    else:
                        j += 1
                # post-condition: i -> ltag, j -> rtag
                raw_form = ' '.join(tokens[i+1: j])

                forms.append(raw_form)
                for phrase in rule:
                    if phrase != raw_form:
                        forms.append(' '.join([ x+'#' for x in phrase.split() ]))
                i = j + 1
            else:  # normal token segment
                token = tokens[i]
                forms.append(token)
                if "-" in token:  # token with hyphen yields extra forms
                    forms.append(' '.join([ x+'#' for x in token.split('-') ])) # 'T-SHIRT' -> 'T# SHIRT#'
                    forms.append(token.replace('-', '') + '#') # 'T-SHIRT' -> 'TSHIRT#'
                i += 1

            sausage_segment = pynini.union(
                *[ pynini.accep(x, token_type=symtab) for x in forms ]
            ).optimize()
            sausage += sausage_segment
        hyp_fst = sausage.optimize()


        # Utterance-Level evaluation
        alignment = edit_transducer.compute_alignment(ref_fst, hyp_fst)
        C, S, I, D = 0, 0, 0, 0  # Cor, Sub, Ins, Del
        edits = []
        distance = 0.0
        for state in alignment.states():
            for arc in alignment.arcs(state):
                i, o = arc.ilabel, arc.olabel
                if i != 0 and o != 0 and not SymbolEQ(symtab, i, o):
                    S += 1
                    e = 'S'
                    distance += 1.0
                elif i != 0 and o == 0:
                    D += 1
                    e = 'D'
                    distance += 1.0
                elif i == 0 and o != 0:
                    I += 1
                    e = 'I'
                    distance += 1.0
                elif SymbolEQ(symtab, i, o):
                    C += 1
                    e = 'C'
                else:
                    raise RuntimeError
                edits.append(e)
                # print('i:', i, symtab.find(i), 'o:', o, symtab.find(o), f'[{e}]')
        #assert(distance == edit_transducer.compute_distance(ref_fst, sausage))

        utt_ter = ComputeTokenErrorRate(C, S, I, D)
        print(F'{{"uid":{uid}, "score":{-distance}, "ter":{utt_ter:.2f}, "cor":{C}, "sub":{S}, "ins":{I}, "del":{D}}}', file=fo)

        hyp_aux = alignment.string(token_type = symtab)
        PrintPrettyAlignment(hyp, hyp_aux, ref, edits, fo)

        if utt_ter > 0:
            stats.num_utts_with_error += 1

        stats.C += C
        stats.S += S
        stats.I += I
        stats.D += D

        ndone += 1
        if ndone % args.logk == 0:
            logging.info(f'{ndone} utts evaluated.')
    logging.info(f'{ndone} utts evaluated in total.')

    # Corpus-Level evaluation
    stats.token_error_rate = ComputeTokenErrorRate(stats.C, stats.S, stats.I, stats.D)
    stats.sentence_error_rate = ComputeSentenceErrorRate(stats.num_utts_with_error, stats.num_eval_utts)

    print(stats.to_json())
    print(stats.to_kaldi())

    print(stats.to_summary(), file=fo)
    fo.close()
