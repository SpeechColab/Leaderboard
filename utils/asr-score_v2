#!/usr/bin/env python3
# coding=utf8

# Copyright  2021  Jiayu DU
import pynini
import sys
import os
import argparse
import json
import csv
import logging
from pynini.lib.edit_transducer import EditTransducer
from pynini.lib import pynutil,utf8
# import utils
logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='[%(levelname)s] %(message)s')

DEBUG = None

def PrettyPrintAlignment(iex, oex, raw, error_dict, stream = sys.stderr):
    def get_token_str(token):
        if token == None:
            return "*"
        return token
    
    def is_double_width_char(ch):
        if (ch >= '\u4e00') and (ch <= '\u9fa5'): # codepoint ranges for Chinese chars
            return True
        else:
            return False
    
    def display_width(token_str):
        m = 0
        for c in token_str:
            if is_double_width_char(c):
                m += 2
            else:
                m += 1
        return m
    P = '  RAW  : '
    R = '  REF  : '
    H = '  HYP  : '
    E = '  EDIT : '
    iex = iex.strip().split(" ")
    oex = oex.strip().split(" ")
    l1 = 0
    l2 = 0
    for i in range(len(error_dict)):
        if error_dict[i] != 'D'and l1 < len(iex):
            h = iex[l1]
            l1+=1
        else:
            h = '*'
        if error_dict[i] != 'I' and l2 < len(oex):
            r = oex[l2]
            l2+=1
        else:
            r = '*'  
        e = error_dict[i] if error_dict[i] != 'C' else ''
        nr, nh, ne = display_width(r), display_width(h), display_width(e)
        n = max(nr, nh, ne) + 1
        R += r + ' ' * (n-nr)
        H += h + ' ' * (n-nh)
        E += e + ' ' * (n-ne)

    P = P + raw   
    print(R, file=stream)
    print(P, file=stream)
    print(H, file=stream)
    print(E, file=stream)



def CountEdits(count):
    c = count[0]
    s = count[1]
    i = count[2]
    d = count[3]
    return (c, s, i, d)

def ComputeTokenErrorRate(c, s, i, d):
    return 100.0 * (s + d + i) / (s + d + c)

def ComputeSentenceErrorRate(num_err_utts, num_utts):
    assert(num_utts != 0)
    return 100.0 * num_err_utts / num_utts


class EvaluationResult:
    def __init__(self):
        self.num_ref_utts = 0
        self.num_hyp_utts = 0
        self.num_eval_utts = 0 # seen in both ref & hyp
        self.num_hyp_without_ref = 0

        self.C = 0
        self.S = 0
        self.I = 0
        self.D = 0
        self.token_error_rate = 0.0

        self.num_utts_with_error = 0
        self.sentence_error_rate = 0.0
    
    def to_json(self):
        return json.dumps(self.__dict__)
    
    def to_kaldi(self):
        info = (
            F'%WER {self.token_error_rate:.2f} [ {self.S + self.D + self.I} / {self.C + self.S + self.D}, {self.I} ins, {self.D} del, {self.S} sub ]\n'
            F'%SER {self.sentence_error_rate:.2f} [ {self.num_utts_with_error} / {self.num_eval_utts} ]\n'
        )
        return info
    
    def to_summary(self):
        #return json.dumps(self.__dict__, indent=4)
        summary = (
            '==================== Overall Statistics ====================\n'
            F'num_ref_utts: {self.num_ref_utts}\n'
            F'num_hyp_utts: {self.num_hyp_utts}\n'
            F'num_hyp_without_ref: {self.num_hyp_without_ref}\n'
            F'num_eval_utts: {self.num_eval_utts}\n'
            F'sentence_error_rate: {self.sentence_error_rate:.2f}%\n'
            F'token_error_rate: {self.token_error_rate:.2f}%\n'
            F'token_stats:\n'
            F'  - tokens:{self.C + self.S + self.D:>7}\n'
            F'  - edits: {self.S + self.I + self.D:>7}\n'
            F'  - cor:   {self.C:>7}\n'
            F'  - sub:   {self.S:>7}\n'
            F'  - ins:   {self.I:>7}\n'
            F'  - del:   {self.D:>7}\n'
            '============================================================\n'
        )
        return summary


class Utterance:
    def __init__(self, uid, text):
        self.uid = uid
        self.text = text


def LoadUtterances(filepath, format):
    utts = {}
    if format == 'text': # utt_id word1 word2 ...
        with open(filepath, 'r', encoding='utf8') as f:
            for line in f:
                line = line.strip()
                if line:
                    cols = line.split(maxsplit=1)
                    assert(len(cols) == 2 or len(cols) == 1)
                    uid = cols[0]
                    text = cols[1] if len(cols) == 2 else ''
                    if utts.get(uid) != None:
                        raise RuntimeError(F'Found duplicated utterence id {uid}')
                    utts[uid] = Utterance(uid, text)
    else:
        raise RuntimeError(F'Unsupported text format {format}')
    return utts


def tokenize_text(text, tokenizer):
    if tokenizer == 'whitespace':
        return text.split()
    elif tokenizer == 'char':
        return [ ch for ch in ''.join(text.split()) ]
    else:
        raise RuntimeError(F'ERROR: Unsupported tokenizer {tokenizer}')

def get_abs_path(rel_path):
    """
    Get absolute path
    Args:
        rel_path: relative path to this file
        
    Returns absolute path
    """
    return os.path.dirname(os.path.abspath(__file__)) + '/' + rel_path


def load_labels(abs_path):
    """
    loads relative path file as dictionary
    Args:
        abs_path: absolute path
    Returns dictionary of mappings
    """
    label_tsv = open(abs_path, encoding="utf-8")
    labels = list(csv.reader(label_tsv, delimiter="\t"))
    return labels
class LevenshteinDistance(EditTransducer):
  """Edit transducer augmented with a distance calculator."""

  def distance(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> float:
    """Computes minimum distance.
    This method computes, for a pair of input/output strings or acceptors, the
    minimum edit distance according to the underlying edit transducer.
    Args:
      iexpr: input string or acceptor.
      oexpr: output string or acceptor.
    Returns:
      Minimum edit distance according to the edit transducer.
    """
    lattice = self.create_lattice(iexpr, oexpr)
    # The shortest cost from all final states to the start state is
    # equivalent to the cost of the shortest path.
    start = lattice.start()
    return float(pynini.shortestdistance(lattice, reverse=True)[start])
  
  # Jiayu
  def compute_alignment(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> pynini.FstLike:
    lattice = self.create_lattice(iexpr, oexpr)
    alignment = pynini.shortestpath(lattice, nshortest=1, unique=True).optimize()
    return alignment


def print_symbol_table(symbol_table):
  print('SYMTAB:')
  for k in range(symbol_table.num_symbols()):
    sym = symbol_table.find(k)
    assert(symbol_table.find(sym) == k) # symbol table's find can be used for bi-directional lookup (id <-> sym)
    print(k, sym)
  print()


def make_symbol_table(vocabulary):
  symbol_table = pynini.SymbolTable()
  for x in ['<epsilon>'] + vocabulary:
    symbol_table.add_symbol(x)
  #symbol_table.write_text('symbol_table.txt')
  return symbol_table

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # optional
    parser.add_argument('--tokenizer', choices=['whitespace', 'char'], default='whitespace', help='whitespace for WER, char for CER')
    parser.add_argument('--ref-format', choices=['text'], default='text', help='reference format, first col is utt_id, the rest is text')
    parser.add_argument('--hyp-format', choices=['text'], default='text', help='hypothesis format, first col is utt_id, the rest is text')
    # required
    parser.add_argument('--ref', type=str, required=True, help='input reference file')
    parser.add_argument('--hyp', type=str, required=True, help='input hypothesis file')

    parser.add_argument('result_file', type=str)
    args = parser.parse_args()
    logging.info(args)

    ref_utts = LoadUtterances(args.ref, args.ref_format)
    hyp_utts = LoadUtterances(args.hyp, args.hyp_format)

    r = EvaluationResult()

    # check valid utterances in hyp that have matched non-empty reference
    eval_utts = []
    r.num_hyp_without_ref = 0
    for uid in sorted(hyp_utts.keys()):
        if uid in ref_utts.keys(): 
            if ref_utts[uid].text.strip(): # non-empty reference
                eval_utts.append(uid)
            else:
                logging.warn(F'Found {uid} with empty reference, skipping...')
        else:
            logging.warn(F'Found {uid} without reference, skipping...')
            r.num_hyp_without_ref += 1

    r.num_hyp_utts = len(hyp_utts)
    r.num_ref_utts = len(ref_utts)
    r.num_eval_utts = len(eval_utts)
    synonyms = load_labels(get_abs_path("synonym.tsv"))
    syn_dict = dict()
    syn_tokens = []
    syn_key = []
    line_counter = 0
    symtab = pynini.SymbolTable()
    symtab.add_symbol('<epsilon>')
    syn_tagger = pynini.accep("",token_type =symtab)
    syn_verb = pynini.accep("",token_type =symtab)
    for phrases in synonyms:
        key_name = "<SYN" + str(line_counter+1) + ">"
        
        syn_key.append(key_name)
        symtab.add_symbol(key_name)
        key_graph = pynini.accep(key_name, token_type =symtab)
        line_counter +=1
        temp_tokens = []
        temp_graph = pynini.accep("",token_type =symtab)
        for phrase in phrases:
            temp_tokens.append(phrase)            
            syn_tokens+=phrase.split()
            ttemp_graph = pynini.accep("",token_type =symtab)
            if len(phrase.split())>0:
              for word in phrase.split():
                symtab.add_symbol(word)
                ttemp_graph+=pynini.accep(word,token_type=symtab)
              temp_graph|=ttemp_graph
            else:
                symtab.add_symbol(phrase)
                temp_graph|=pynini.accep(phrase,token_type=symtab)
        syn_tagger|= pynini.cross(temp_graph,key_graph)
        syn_verb |=  pynini.cross(key_graph, temp_graph)
        syn_dict[key_name] = temp_tokens
        vocab = list(set(syn_key))
    with open(args.result_file, 'w+', encoding='utf8') as fo:
        for uid in eval_utts:
            ref = ref_utts[uid].text
            hyp = hyp_utts[uid].text
            ref_tokens = ref.strip().split()
            hyp_tokens = hyp.strip().split()
            symtab1 = symtab.copy()
            vocab1 = list(set(vocab + ref_tokens + hyp_tokens))
            for word in vocab1:
              symtab1.add_symbol(word)
            for word in ref_tokens: # a-b cases
              if "-" in word:
                words = word.split("-")
                word0 = words[0]
                word1 = words[0]
                for i in range(1,len(words)):
                  word0+=" " + words[i]
                  word1+=words[i]
                vocab1.append(word)
                vocab1.append(word0)
                vocab1.append(word1)
                symtab1.add_symbol(word)
                symtab1.add_symbol(word0)
                symtab1.add_symbol(word1)
            alpha = [pynini.accep(token,token_type = symtab1) for token in vocab1]
            hyp_fst = pynini.accep("",token_type =symtab1)
            ref_fst = pynini.accep("",token_type =symtab1)
            SIGMA = pynini.union(*alpha).optimize()
            SIGMA = pynini.closure(SIGMA)
            tagger = pynini.cdrewrite(syn_tagger,"","",SIGMA)
            verb = pynini.cdrewrite(syn_verb,"","",SIGMA)
            for word in ref_tokens:
              ref_fst +=pynini.accep(word, token_type =symtab1)
            
            for word in hyp_tokens:
              hyp_fst +=pynini.accep(word, token_type =symtab1)
            hyp_fst = hyp_fst.optimize()
            tagged_lattice = (hyp_fst @ tagger).optimize()
            tagged_text = pynini.shortestpath(tagged_lattice, nshortest=1, unique=True).string(token_type = symtab1)
            tagged_tokens = tagged_text.split()
            tagged_fst = pynini.accep("",token_type =symtab1)
            for word in tagged_tokens:
              tagged_fst +=pynini.accep(word, token_type =symtab1)
            verb_lattice = (tagged_fst @ verb).optimize()
            verb_text = pynini.shortestpath(verb_lattice, nshortest=1, unique=True).string(token_type = symtab1)         
            distance = LevenshteinDistance(alphabet = alpha)
            score = distance.distance(verb_lattice,ref_fst)
            alignment = distance.compute_alignment(verb_lattice,ref_fst)
            hyp_v2 = alignment.string(token_type=symtab1)
            C, S, I, D = 0, 0, 0, 0
            error_dict = []
            for state in alignment.states():
                for arc in alignment.arcs(state):
                    i, o = arc.ilabel, arc.olabel
                    if i != 0 and o != 0 and i != o:
                        S += 1
                        error_dict.append("S")
                    elif i != 0 and o == 0:
                        D += 1
                        error_dict.append("D")
                    elif i == 0 and o != 0:
                        I += 1
                        error_dict.append("I")
                    elif i == o:
                        C += 1
                        error_dict.append("C")
                    else:
                        raise RuntimeError
            utt_ter = ComputeTokenErrorRate(C, S, I, D)
            
            # utt-level evaluation result
            print(F'{{"uid":{uid}, "score":{score}, "ter":{utt_ter:.2f}, "cor":{C}, "sub":{S}, "ins":{I}, "del":{D}}}', file=fo)
            PrettyPrintAlignment(hyp_v2,ref,hyp,error_dict, fo)

            r.C += C
            r.S += S
            r.I += I
            r.D += D

            if utt_ter > 0:
                r.num_utts_with_error += 1

        # corpus level evaluation result
        r.sentence_error_rate = ComputeSentenceErrorRate(r.num_utts_with_error, r.num_eval_utts)
        r.token_error_rate = ComputeTokenErrorRate(r.C, r.S, r.I, r.D)

        print(r.to_summary(), file=fo)

    print(r.to_json())
    print(r.to_kaldi())

