#!/usr/bin/env python3
# coding=utf8

# Copyright  2021  Jiayu DU
import pynini
import sys
import os
import argparse
import json
import csv
import logging
from typing import Iterable, List
# from pynini.lib.edit_transducer import EditTransducer
from pynini.lib import pynutil,utf8
# import utils
logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='[%(levelname)s] %(message)s')

DEBUG = None

def PrettyPrintAlignment(iex, oex, raw, error_dict, stream = sys.stderr):
    def get_token_str(token):
        if token == None:
            return "*"
        return token
    
    def is_double_width_char(ch):
        if (ch >= '\u4e00') and (ch <= '\u9fa5'): # codepoint ranges for Chinese chars
            return True
        else:
            return False
    
    def display_width(token_str):
        m = 0
        for c in token_str:
            if is_double_width_char(c):
                m += 2
            else:
                m += 1
        return m
    P = '  HYP  : '
    R = '  REF  : '
    H = '  HYP# : '
    E = '  EDIT : '
    iex = iex.strip().split(" ")
    oex = oex.strip().split(" ")
    l1 = 0
    l2 = 0
    for i in range(len(error_dict)):
        if error_dict[i] != 'D'and l1 < len(iex):
            h = iex[l1]
            l1+=1
        else:
            h = '*'
        if error_dict[i] != 'I' and l2 < len(oex):
            r = oex[l2]
            l2+=1
        else:
            r = '*'  
        e = error_dict[i] if error_dict[i] != 'C' else ''
        nr, nh, ne = display_width(r), display_width(h), display_width(e)
        n = max(nr, nh, ne) + 1
        R += r + ' ' * (n-nr)
        H += h + ' ' * (n-nh)
        E += e + ' ' * (n-ne)

    P = P + raw   
   
    print(P, file=stream)
    print(H, file=stream)
    print(R, file=stream)
    print(E, file=stream)



def CountEdits(count):
    c = count[0]
    s = count[1]
    i = count[2]
    d = count[3]
    return (c, s, i, d)

def ComputeTokenErrorRate(c, s, i, d):
    return 100.0 * (s + d + i) / (s + d + c)

def ComputeSentenceErrorRate(num_err_utts, num_utts):
    assert(num_utts != 0)
    return 100.0 * num_err_utts / num_utts

class EditTransducer:
  """Factored edit transducer.
  This class stores the two factors of an finite-alphabet edit transducer and
  supports insertion, deletion, and substitution operations with user-specified
  costs.
  Note that the cost of substitution must be less than the cost of insertion
  plus the cost of deletion or no optimal path will include substitution.
  One can impose an upper bound on the number of permissible edits by
  setting a non-zero value for `bound`. This often results in substantial
  improvements in performance.
  """
  # Reserved labels for edit operations.
  DELETE = "<delete>"
  INSERT = "<insert>"
  SUBSTITUTE = "<substitute>"


  def __init__(self,
               symbol_table,
               vocab: Iterable[str],
               vocab_aux: Iterable[str],
               insert_cost: float = 1.0,
               delete_cost: float = 1.0,
               substitute_cost: float = 1.0,
               bound: int = 0):
    """Constructor.
    Args:
      alphabet: edit alphabet (an iterable of strings).
      insert_cost: the cost for the insertion operation.
      delete_cost: the cost for the deletion operation.
      substitute_cost: the cost for the substitution operation.
      bound: the number of permissible edits, or `0` (the default) if there
          is no upper bound.
    """
    # Left factor; note that we divide the edit costs by two because they also
    # will be incurred when traversing the right factor.
    sigma = pynini.union(
      *[ pynini.accep(token, token_type = symbol_table) for token in vocab ], 
    ).optimize()

    insert = pynutil.insert(f"[{self.INSERT}]", weight=insert_cost / 2)
    delete = pynini.cross(
      sigma,
      pynini.accep(f"[{self.DELETE}]", weight=delete_cost / 2)
    )
    substitute = pynini.cross(
      sigma, pynini.accep(f"[{self.SUBSTITUTE}]", weight=substitute_cost / 2)
    )

    edit = pynini.union(insert, delete, substitute).optimize()

    if bound:
      sigma_star = pynini.closure(sigma)
      self._e_i = sigma_star.copy()
      for _ in range(bound):
        self._e_i.concat(edit.ques).concat(sigma_star)
    else:
      self._e_i = edit.union(sigma).closure()
    self._e_i.optimize()
    self._e_o = EditTransducer._right_factor(self._e_i, sigma, vocab_aux)

  @staticmethod
  def _right_factor(ifst: pynini.Fst, sigma, vocab_aux) -> pynini.Fst:
    """Constructs the right factor from the left factor."""
    # Ts constructed by inverting the left factor (i.e., swapping the input and
    # output labels), then swapping the insert and delete labels on what is now
    # the input side.
    ofst = pynini.invert(ifst)
    syms = pynini.generated_symbols()
    insert_label = syms.find(EditTransducer.INSERT)
    delete_label = syms.find(EditTransducer.DELETE)
    pairs = [(insert_label, delete_label), (delete_label, insert_label)]
    right_factor = ofst.relabel_pairs(ipairs=pairs)

    # Jiayu
    # auxiliary paths allow 0-cost matches between raw token and auxiliary token, e.g.: 'I' -> 'I#', 'AM' -> 'AM#'
    right_factor_aux = pynini.union(*
      [
        pynini.cross(
          pynini.accep(token,       token_type = symtab),
          pynini.accep(token + '#', token_type = symtab),
        ) for token in vocab_aux
      ]
    ).optimize().closure()

    R = pynini.union(right_factor, right_factor_aux).closure().optimize()
    #print('R_start:', R.start())
    #print('R:', R)

    return R

  @staticmethod
  def check_wellformed_lattice(lattice: pynini.Fst) -> None:
    """Raises an error if the lattice is empty.
    Args:
      lattice: A lattice FST.
    Raises:
      Error: Lattice is empty.
    """
    if lattice.start() == pynini.NO_STATE_ID:
      raise Error("Lattice is empty")

  def create_lattice(self, iexpr: pynini.FstLike,
                     oexpr: pynini.FstLike) -> pynini.Fst:
    """Creates edit lattice for a pair of input/output strings or acceptors.
    Args:
      iexpr: input string or acceptor
      oexpr: output string or acceptor.
    Returns:
      A lattice FST.
    """

    lattice = (iexpr @ self._e_i) @ (self._e_o @ oexpr)
    EditTransducer.check_wellformed_lattice(lattice)
    return lattice
class EvaluationResult:
    def __init__(self):
        self.num_ref_utts = 0
        self.num_hyp_utts = 0
        self.num_eval_utts = 0 # seen in both ref & hyp
        self.num_hyp_without_ref = 0

        self.C = 0
        self.S = 0
        self.I = 0
        self.D = 0
        self.token_error_rate = 0.0

        self.num_utts_with_error = 0
        self.sentence_error_rate = 0.0
    
    def to_json(self):
        return json.dumps(self.__dict__)
    
    def to_kaldi(self):
        info = (
            F'%WER {self.token_error_rate:.2f} [ {self.S + self.D + self.I} / {self.C + self.S + self.D}, {self.I} ins, {self.D} del, {self.S} sub ]\n'
            F'%SER {self.sentence_error_rate:.2f} [ {self.num_utts_with_error} / {self.num_eval_utts} ]\n'
        )
        return info
    
    def to_summary(self):
        #return json.dumps(self.__dict__, indent=4)
        summary = (
            '==================== Overall Statistics ====================\n'
            F'num_ref_utts: {self.num_ref_utts}\n'
            F'num_hyp_utts: {self.num_hyp_utts}\n'
            F'num_hyp_without_ref: {self.num_hyp_without_ref}\n'
            F'num_eval_utts: {self.num_eval_utts}\n'
            F'sentence_error_rate: {self.sentence_error_rate:.2f}%\n'
            F'token_error_rate: {self.token_error_rate:.2f}%\n'
            F'token_stats:\n'
            F'  - tokens:{self.C + self.S + self.D:>7}\n'
            F'  - edits: {self.S + self.I + self.D:>7}\n'
            F'  - cor:   {self.C:>7}\n'
            F'  - sub:   {self.S:>7}\n'
            F'  - ins:   {self.I:>7}\n'
            F'  - del:   {self.D:>7}\n'
            '============================================================\n'
        )
        return summary


class Utterance:
    def __init__(self, uid, text):
        self.uid = uid
        self.text = text


def LoadUtterances(filepath, format):
    utts = {}
    if format == 'text': # utt_id word1 word2 ...
        with open(filepath, 'r', encoding='utf8') as f:
            for line in f:
                line = line.strip()
                if line:
                    cols = line.split(maxsplit=1)
                    assert(len(cols) == 2 or len(cols) == 1)
                    uid = cols[0]
                    text = cols[1] if len(cols) == 2 else ''
                    if utts.get(uid) != None:
                        raise RuntimeError(F'Found duplicated utterence id {uid}')
                    utts[uid] = Utterance(uid, text)
    else:
        raise RuntimeError(F'Unsupported text format {format}')
    return utts


def tokenize_text(text, tokenizer):
    if tokenizer == 'whitespace':
        return text.split()
    elif tokenizer == 'char':
        return [ ch for ch in ''.join(text.split()) ]
    else:
        raise RuntimeError(F'ERROR: Unsupported tokenizer {tokenizer}')

def get_abs_path(rel_path):
    """
    Get absolute path
    Args:
        rel_path: relative path to this file
        
    Returns absolute path
    """
    return os.path.dirname(os.path.abspath(__file__)) + '/' + rel_path


def load_labels(abs_path):
    """
    loads relative path file as dictionary
    Args:
        abs_path: absolute path
    Returns dictionary of mappings
    """
    label_tsv = open(abs_path, encoding="utf-8")
    labels = list(csv.reader(label_tsv, delimiter="\t"))
    return labels
class LevenshteinDistance(EditTransducer):
  """Edit transducer augmented with a distance calculator."""

  def distance(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> float:
    """Computes minimum distance.
    This method computes, for a pair of input/output strings or acceptors, the
    minimum edit distance according to the underlying edit transducer.
    Args:
      iexpr: input string or acceptor.
      oexpr: output string or acceptor.
    Returns:
      Minimum edit distance according to the edit transducer.
    """
    lattice = self.create_lattice(iexpr, oexpr)
    # The shortest cost from all final states to the start state is
    # equivalent to the cost of the shortest path.
    start = lattice.start()
    return float(pynini.shortestdistance(lattice, reverse=True)[start])
  
  # Jiayu
  def compute_alignment(self, iexpr: pynini.FstLike, oexpr: pynini.FstLike) -> pynini.FstLike:
    lattice = self.create_lattice(iexpr, oexpr)
    alignment = pynini.shortestpath(lattice, nshortest=1, unique=True).optimize()
    return alignment


def print_symbol_table(symbol_table):
  print('SYMTAB:')
  for k in range(symbol_table.num_symbols()):
    sym = symbol_table.find(k)
    assert(symbol_table.find(sym) == k) # symbol table's find can be used for bi-directional lookup (id <-> sym)
    print(k, sym)
  print()


def make_symbol_table(vocabulary):
  symbol_table = pynini.SymbolTable()
  for x in ['<epsilon>'] + vocabulary:
    symbol_table.add_symbol(x)
  #symbol_table.write_text('symbol_table.txt')
  return symbol_table

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # optional
    parser.add_argument('--tokenizer', choices=['whitespace', 'char'], default='whitespace', help='whitespace for WER, char for CER')
    parser.add_argument('--ref-format', choices=['text'], default='text', help='reference format, first col is utt_id, the rest is text')
    parser.add_argument('--hyp-format', choices=['text'], default='text', help='hypothesis format, first col is utt_id, the rest is text')
    # required
    parser.add_argument('--ref', type=str, required=True, help='input reference file')
    parser.add_argument('--hyp', type=str, required=True, help='input hypothesis file')

    parser.add_argument('result_file', type=str)
    args = parser.parse_args()
    logging.info(args)

    ref_utts = LoadUtterances(args.ref, args.ref_format)
    hyp_utts = LoadUtterances(args.hyp, args.hyp_format)

    r = EvaluationResult()

    # check valid utterances in hyp that have matched non-empty reference
    eval_utts = []
    r.num_hyp_without_ref = 0
    for uid in sorted(hyp_utts.keys()):
        if uid in ref_utts.keys(): 
            if ref_utts[uid].text.strip(): # non-empty reference
                eval_utts.append(uid)
            else:
                logging.warn(F'Found {uid} with empty reference, skipping...')
        else:
            logging.warn(F'Found {uid} without reference, skipping...')
            r.num_hyp_without_ref += 1

    r.num_hyp_utts = len(hyp_utts)
    r.num_ref_utts = len(ref_utts)
    r.num_eval_utts = len(eval_utts)
    synonyms = load_labels(get_abs_path("synonym.tsv"))
    syn_dict = dict()
    syn_tokens = []
    syn_key = []
    line_counter = 0
    symtab = pynini.SymbolTable()
    symtab.add_symbol('<epsilon>')
    symtab.add_symbol('<')
    symtab.add_symbol('>')
    symtab.add_symbol('#')
    syn_tagger = pynini.accep("",token_type =symtab)
    syn_flag = False
    # syn_verb = pynini.accep("",token_type =symtab)
    for phrases in synonyms:
        key_name = "<SYN" + str(line_counter+1) + ">"
        
        syn_key.append(key_name)
        symtab.add_symbol(key_name)
        key_graph = pynini.accep(key_name, token_type =symtab)
        line_counter +=1
        temp_tokens = []
        temp_graph = pynini.accep("",token_type =symtab)
        temp_syn_flag = False
        for phrase in phrases:
            temp_tokens.append(phrase)            
            syn_tokens+=phrase.split()
            ttemp_graph = pynini.accep("",token_type =symtab)
            if len(phrase.split())>0:
              for word in phrase.split():
                symtab.add_symbol(word)
                symtab.add_symbol(word + '#')
                ttemp_graph+=pynini.accep(word,token_type=symtab)
              if not temp_syn_flag:
                temp_graph = ttemp_graph
                temp_syn_flag = True
              else:
                temp_graph|=ttemp_graph
            else:
                symtab.add_symbol(phrase)
                symtab.add_symbol(phrase + '#')
                temp_graph|=pynini.accep(phrase,token_type=symtab)
        if not syn_flag:
            syn_tagger= (
                pynutil.insert(key_graph) + temp_graph + pynutil.insert(key_graph)
            )
            syn_flag = True
        else:
            syn_tagger |= (              
                pynutil.insert(key_graph) + temp_graph + pynutil.insert(key_graph)
            )
        syn_dict[key_name] = temp_tokens
        vocab_syn = list(set(syn_tokens))
        syn_tagger.optimize()
        # SIGMA=pynini.closure(utf8.VALID_UTF8_CHAR)
        # tagger = pynini.cdrewrite(syn_tagger,"","",SIGMA)
        # alpha = [pynini.accep(token,token_type = symtab) for token in vocab_syn]
        # SIGMA = pynini.union(*alpha).optimize()
        # text = pynini.closure(SIGMA)
        # syn_verb |=  (
        #     # pynini.cross(key_graph, temp_graph)
        #     pynutil.delete(key_graph)
        #     + text
        #     + pynutil.delete(key_graph)
        # )
    symtab2 = symtab.copy()
    temp_vocab = []
    for uid in eval_utts:
        ref = ref_utts[uid].text
        hyp = hyp_utts[uid].text
        ref_tokens = ref.strip().split()
        hyp_tokens = hyp.strip().split()
        for word in ref_tokens:
            temp_vocab.append(word)
            symtab2.add_symbol(word)
            if "-" in word:
                words = word.split("-")
                word0 = words[0]
                word1 = words[0]
                symtab2.add_symbol(words[0])
                for i in range(1,len(words)):
                  word0+=" " + words[i]
                  word1+=words[i]
                  symtab2.add_symbol(words[i])
                temp_vocab.append(word.strip())
                temp_vocab.append(word0.strip())
                temp_vocab.append(word1.strip())
                symtab2.add_symbol(word.strip())
                symtab2.add_symbol(word0.strip())
                symtab2.add_symbol(word1.strip())
        for word in hyp_tokens:
            temp_vocab.append(word)
            symtab2.add_symbol(word)
            if "-" in word:
                words = word.split("-")
                word0 = words[0]
                word1 = words[0]
                symtab2.add_symbol(words[0])
                for i in range(1,len(words)):
                  word0+=" " + words[i]
                  word1+=words[i]
                  symtab2.add_symbol(words[i])
                temp_vocab.append(word.strip())
                temp_vocab.append(word0.strip())
                temp_vocab.append(word1.strip())
                symtab2.add_symbol(word.strip())
                symtab2.add_symbol(word0.strip())
                symtab2.add_symbol(word1.strip())
    
    temp_vocab = list(set(temp_vocab + syn_tokens + syn_key))
    alpha = [pynini.accep(token,token_type = symtab2) for token in temp_vocab]
    SIGMA = pynini.union(*alpha).optimize()
    SIGMA = pynini.closure(SIGMA)
    tagger = pynini.cdrewrite(syn_tagger,"","",SIGMA)
    with open(args.result_file, 'w+', encoding='utf8') as fo:
        for uid in eval_utts:
            ref = ref_utts[uid].text
            hyp = hyp_utts[uid].text
            ref_tokens = ref.strip().split(" ")
            
            hyp_tokens = hyp.strip().split(" ")
            symtab1 = symtab.copy()
            vocab1 = list(set(syn_key + vocab_syn + ref_tokens + hyp_tokens))
            for word in vocab1:
              symtab1.add_symbol(word)
            
            for word in hyp_tokens: # a-b cases
              if "-" in word:
                words = word.split("-")
                word0 = words[0]
                word1 = words[0]
                symtab1.add_symbol(words[0])
                for i in range(1,len(words)):
                  word0+=" " + words[i]
                  word1+=words[i]
                  symtab1.add_symbol(words[i])
                vocab1.append(word)
                vocab1.append(word0)
                vocab1.append(word1)
                symtab1.add_symbol(word)
                symtab1.add_symbol(word0)
                symtab1.add_symbol(word1)
            alpha = [pynini.accep(token,token_type = symtab1) for token in vocab1]
            hyp_fst = pynini.accep("",token_type =symtab1)
            ref_fst = pynini.accep("",token_type =symtab1)
            # SIGMA = pynini.union(*alpha).optimize()
            # SIGMA = pynini.closure(SIGMA)
            # tagger = pynini.cdrewrite(syn_tagger,"","",SIGMA)
            # verb = pynini.cdrewrite(syn_verb,"","",SIGMA)
            for word in ref_tokens:
              ref_fst +=pynini.accep(word, token_type =symtab1)
            
            for word in hyp_tokens:
              hyp_fst +=pynini.accep(word, token_type =symtab1)
            hyp_fst = hyp_fst.optimize()
            tagged_lattice = (hyp_fst @ tagger).optimize()
            tagged_text = pynini.shortestpath(tagged_lattice, nshortest=1, unique=True).string(token_type = symtab1)
            tagged_tokens = tagged_text.split()
            tagged_fst = pynini.accep("",token_type =symtab1)
            
            # print_symbol_table(symtab1)
            i = 0
            while i < len(tagged_tokens):
                word = tagged_tokens[i]
                if "<SYN" not in word:
                    if "-" not in word:
                        tagged_fst +=pynini.accep(word, token_type =symtab1)
                    else:
                        words = word.split("-")
                        word1 = words[0]
                        connect_graph0 = pynini.accep(words[0],token_type =symtab1)
                        connect_graph = pynini.accep(word,token_type =symtab1)
                        
                        flag0 = False
                        for k in range(1,len(words)):
                            connect_graph0 +=pynini.accep(words[k],token_type =symtab1)
                            word1+=words[k]
                        symtab1.add_symbol(word1)
                        connect_graph1 = pynini.accep(word1,token_type =symtab1)
                        tagged_fst +=pynini.union(connect_graph,connect_graph0,connect_graph1).optimize()
                    i = i+1
                else:
                    key_name = word
                    list1 = syn_dict[key_name]
                    part_word = ""
                    i = i+1
                    while "<SYN" not in tagged_tokens[i]:
                        part_word += tagged_tokens[i] + " "
                        i = i+1                   
                    if "<SYN" in word:
                        i = i + 1
                    temp_graph = pynini.accep("",token_type =symtab1)
                    temp_flag = False
                    for synword in list1:
                        part_word = part_word.strip()
                        if synword == part_word:
                            ttemp_graph = pynini.accep("",token_type =symtab1)
                            if len(synword.split())>1:
                                synwords = synword.split()
                                for j in range(len(synwords)):
                                    ttemp_graph+=pynini.accep(synwords[j],token_type = symtab1)
                                if not temp_flag:
                                    temp_graph = ttemp_graph.optimize()
                                    temp_flag = True
                                else:
                                    temp_graph|=ttemp_graph.optimize()
                            else:
                                ttemp_graph = pynini.accep(synword,token_type = symtab1)
                                if not temp_flag:
                                    temp_graph = ttemp_graph.optimize()
                                    temp_flag = True
                                else:
                                    temp_graph|=ttemp_graph.optimize()
                        else:
                            ttemp_graph = pynini.accep("",token_type =symtab1)
                            if len(synword.split())>1:
                                synwords = synword.split()
                                # print(synwords)
                                for j in range(len(synwords)):
                                    ttemp_graph+=pynini.accep(synwords[j] + "#",token_type = symtab1)                                                               
                                if not temp_flag:
                                    temp_graph = ttemp_graph.optimize()
                                    temp_flag = True
                                else:
                                    temp_graph|=ttemp_graph.optimize()
                            else:
                                ttemp_graph = pynini.accep(synword + "#",token_type = symtab1)
                                if not temp_flag:
                                    temp_graph = ttemp_graph.optimize()
                                    temp_flag = True
                                else:
                                    temp_graph|=ttemp_graph.optimize()
                    tagged_fst +=temp_graph.optimize()
            tagged_fst = tagged_fst.optimize()
            # verb_lattice = (tagged_fst @ verb).optimize()
            # verb_text = pynini.shortestpath(verb_lattice, nshortest=1, unique=True).string(token_type = symtab1)  
            # print(verb_text)       
            distance = LevenshteinDistance(
                    symbol_table =  symtab1,            
                    vocab = vocab1,
                    vocab_aux = vocab_syn
                )
            score = distance.distance(ref_fst,tagged_fst)
            alignment = distance.compute_alignment(ref_fst, tagged_fst)
            hyp_v2 = alignment.string(token_type=symtab1)
            error_dict = []
            def are_same(symtab, i1, i2):
                return symtab.find(i1).strip('#') == symtab.find(i2).strip('#')


            C, S, I, D = 0, 0, 0, 0
            for state in alignment.states():
                for arc in alignment.arcs(state):
                    i, o = arc.ilabel, arc.olabel

                    edit = ''
                    if i != 0 and o != 0 and not are_same(symtab1, i, o):
                        S += 1
                        edit = 'S'
                        error_dict.append("S")
                    elif i != 0 and o == 0:
                        D += 1
                        edit = 'D'
                        error_dict.append("D")
                    elif i == 0 and o != 0:
                        I += 1
                        edit = 'I'
                        error_dict.append("I")
                    elif are_same(symtab1, i, o):
                        C += 1
                        edit = 'C'
                        error_dict.append("C")
                    else:
                        raise RuntimeError

                    # print('i:', i, symtab1.find(i), 'o:', o, symtab1.find(o), f'[{edit}]')
            utt_ter = ComputeTokenErrorRate(C, S, I, D)
            # print(error_dict)
            # utt-level evaluation result
            print(F'{{"uid":{uid}, "score":{score}, "ter":{utt_ter:.2f}, "cor":{C}, "sub":{S}, "ins":{I}, "del":{D}}}', file=fo)
            PrettyPrintAlignment(hyp_v2,ref,hyp,error_dict, fo)

            r.C += C
            r.S += S
            r.I += I
            r.D += D

            if utt_ter > 0:
                r.num_utts_with_error += 1

        # corpus level evaluation result
        r.sentence_error_rate = ComputeSentenceErrorRate(r.num_utts_with_error, r.num_eval_utts)
        r.token_error_rate = ComputeTokenErrorRate(r.C, r.S, r.I, r.D)

        print(r.to_summary(), file=fo)

    print(r.to_json())
    print(r.to_kaldi())

